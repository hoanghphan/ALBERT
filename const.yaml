description: albert_test
checkpoint_storage:
  type: gcs
  bucket: determined-ai-dev-test
data:
#  data_dir: /home/hphan/data/glue
    # "The input data dir. Should contain the .tsv files (or other data files) "
    # "for the task."
#  albert_config_file: /home/hphan/data/albert_base/albert_config.json
    # "The config json file corresponding to the pre-trained ALBERT model."
    # "This specifies the model architecture."
#  task_name: MNLI
    # "The name of the task to train."
#  vocab_file: 
    # "The vocabulary file that the ALBERT model was trained on."
#  spm_model_file:
    # "The model file for sentence piece tokenization."
#  output_dir:
    # "The output directory where the model checkpoints will be written."
#  cached_dir:
    # "Path to cached training and dev tfrecord file. "
    # "The file will be generated if not exist."
#  init_checkpoint:  # Initial checkpoint (usually from a pre-trained BERT model).
#  albert_hub_module_handle:  # If set, the ALBERT hub module to use.
#  do_lower_case: # Whether to lower case the input text. Should be True for uncased models and False for cased models.
#  max_seq_length: 512
    # "The maximum total input sequence length after WordPiece tokenization. "
    # "Sequences longer than this will be truncated, and sequences shorter "
    # "than this will be padded."
#  optimizer: adamw
#  do_train: # remove
#  do_eval: # remove
#  do_predict: # remove
hyperparameters:
  batch_size: 1
  train_batch_size: 32
  eval_batch_size: 8
  predict_batch_size: 8
  learning_rate: 5e-5
  train_step: 1000 # remove
  warmup_step: 0 # remove
  save_checkpoints_steps: 1000 # remove
  keep_checkpoints_max: 5 # remove
  iterations_per_loop: 1000 # remove
  use_tpu: False # remove
searcher:
  name: single
  metric: eval_accuracy
  max_steps: 5
  smaller_is_better: false
environment:
  tensorflow: "1.15.0"
  runtime_packages:
    - tensorflow-hub
    - sentencepiece
bind_mounts:
  - host_path: /home/hphan
    container_path: /home/hphan
min_validation_period: 1
max_restarts: 0
